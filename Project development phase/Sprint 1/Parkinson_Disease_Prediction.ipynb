{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2f8cd2",
   "metadata": {},
   "source": [
    "# Image Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7429d",
   "metadata": {},
   "source": [
    "## Import the necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee24efc",
   "metadata": {},
   "source": [
    "##### Note: Download scikit-image for skimage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e8459eb",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import feature\n",
    "from imutils import build_montages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from keras.applications import mobilenet\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb680b",
   "metadata": {},
   "source": [
    "### Path for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6183b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingpath=r\"C:/xampp/htdocs/Website/spiral/training\"\n",
    "testingpath=r\"C:/xampp/htdocs/Website/spiral/testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14da86",
   "metadata": {},
   "source": [
    "## Quantifying Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c489f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_image(image):\n",
    "    features = feature.hog(image, orientations=9,\n",
    "                           pixels_per_cell=(10, 10), \n",
    "                           cells_per_block=(2, 2),\n",
    "                           transform_sqrt=True, \n",
    "                           block_norm=\"L1\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36eb05",
   "metadata": {},
   "source": [
    "## Loading Train Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350812ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(path):\n",
    "    imagePaths = list(paths.list_images(path))\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for imagePath in imagePaths:\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.resize(image, (200, 200))\n",
    "\n",
    "        image=cv2.threshold(image,0,255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        \n",
    "        features = quantify_image(image)\n",
    "\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "    return (np.array(data), np.array(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2e430",
   "metadata": {},
   "source": [
    "### Load the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7381531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading data...\")\n",
    "(X_train, y_train) = load_split(trainingpath)\n",
    "(X_test, y_test) = load_split(testingpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe1227",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3b5fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 12996) (72,)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3db9c3",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967fdb7",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862ef4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO] training model\")\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e127483",
   "metadata": {},
   "source": [
    "## Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6238c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingpath=list(paths.list_images(testingpath))\n",
    "idxs=np.arange(0,len(testingpath))\n",
    "idxs=np.random.choice(idxs,size=(25,),replace=False)\n",
    "images=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fdc6359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in idxs:\n",
    "    image=cv2.imread(testingpath[i])\n",
    "    output=image.copy()\n",
    "        \n",
    "    # load the input image,convert to grayscale and resize\n",
    "    \n",
    "    output=cv2.resize(output,(128,128))\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image=cv2.resize(image,(200,200))\n",
    "    image=cv2.threshold(image,0,255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    #quantify the image and make predictions based on the  extracted feature using last trained random forest\n",
    "    features=quantify_image(image)\n",
    "    preds=model.predict([features])\n",
    "    label=le.inverse_transform(preds)[0]\n",
    "    #the set of output images\n",
    "    if label==\"healthy\":\n",
    "        color=(0,255,0)\n",
    "    else:\n",
    "        color=(0,0,255)\n",
    "        \n",
    "    cv2.putText(output,label,(3,20),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,2)\n",
    "    images.append(output)\n",
    "\n",
    "#creating a montage\n",
    "montage=build_montages(images,(128,128),(5,5))[0]\n",
    "cv2.imshow(\"Output\",montage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d260e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81277910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14  1  4 11]\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions).flatten()\n",
    "print(cm)\n",
    "(tn, fp, fn, tp) = cm\n",
    "accuracy = (tp + tn) / float(cm.sum())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4705a",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95998302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open('parkinson.pkl','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90468350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
